---
title: "Data EDA"
format:
  revealjs:
    margin: 0
execute:
    error: false
    message: false
    warning: false
---

```{r params}

run_lemmatization = FALSE

```



## Scoper data lemmatization and frequency


```{r eval=run_lemmatization}

if (Sys.getenv("USERPROFILE") == "C:\\Users\\internet"){
  
  python_path = paste0("C:\\Users\\internet\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
} else {
  
  python_path = paste0("C:\\Users\\Home\\AppData\\Local",
                       "\\Programs\\Python\\Python312\\python.exe")
}

reticulate::use_python(python_path)

```


```{python import_libraries, eval=run_lemmatization}

import pandas as pd

import os

import time
import stanza
import stopwordsiso as siso
from collections import Counter


# 4. Download Hebrew models (runs once)
stanza.download('he')

# 5. Build the Stanza pipeline for Hebrew
nlp = stanza.Pipeline(
    lang='he',
    processors='tokenize,mwt,pos,lemma',
    dir='D:/stanza_resources',
    use_gpu=False  # set True if you enabled GPU in Colab
)

he_stopwords = siso.stopwords("he")

```


```{python load_data, eval=run_lemmatization}


df = pd.read_csv(os.path.join(os.path.expanduser("~"),
"OneDrive - Bank Of Israel\\current_work\\real estate sentiment\\twitter\\scoper_sample.csv"))

```


```{python define_count_lemmas, eval=run_lemmatization}

def count_lemmas(text):
    all_lemmas = []
    for text in texts:
      doc = nlp(text)
      for sentence in doc.sentences:
          for word in sentence.words:
            if (
                word.upos == 'NOUN'
                # or word.upos == 'PRON'
                and word.lemma not in he_stopwords
            ):
                  all_lemmas.append(word.lemma)

      all_lemmas = [temp_lemma for temp_lemma in all_lemmas if len(temp_lemma) > 3]

      freq = Counter(all_lemmas)

    return(freq)

```


```{python lemmatize_texts, eval=run_lemmatization}

texts = df['content'].dropna().sample(500)

start = time.perf_counter()

lemmas_counter = count_lemmas(texts)

end = time.perf_counter()

print(f"Time taken: {(end - start) / 60 :0.2f} minutes")

```


```{python save_lemmas_count_df, eval=run_lemmatization}


lemmas_count_df = pd.DataFrame(lemmas_counter.items(), columns=['lemma', 'count'])

lemmas_count_df.to_csv(os.path.join(os.path.expanduser("~"),
"OneDrive - Bank Of Israel\\current_work\\real estate sentiment\\twitter\\lemmas_count.csv"), index=False, encoding='utf-8-sig')



```

## Classify scoper

```{r libraries}

library(tidyverse)

library(gt)

devtools::load_all()

```


```{r load_scoper_data}

scoper_sample = read_csv(paste0(Sys.getenv("USERPROFILE"),
                                "\\OneDrive - Bank Of Israel\\current_work",
                                "\\real estate sentiment\\twitter",
                                "\\scoper_sample.csv"))

```



```{r load_lemmas}

lemmas_count = read_csv(paste0(Sys.getenv("USERPROFILE"),
                               "\\OneDrive - Bank Of Israel\\current_work",
                               "\\real estate sentiment",
                               "\\twitter\\lemmas_count.csv"))

lemmas_count = lemmas_count %>% 
  arrange(desc(count))

code_words = lemmas_count %>% 
  slice(1:10) %>% 
  pull(lemma)

code_words = code_words[c(1,3,6,9)]

```


```{r}

scoper_sample = scoper_sample %>% 
  mutate(relevance_ind = map_dbl(content, ~any(str_detect(., code_words[c(1,3,6,9)]))))


scoper_sample %>% 
  filter(is.na(relevance_ind))

```




```{r params}

params = list()

params$stakeholders = c('employees',
                        'unemployed',
                        'employers',
                        'government',
                        'labor unions')

params$topics = c(
  'employment',
  'unemployment',
  'job vacancies',
  'minimum wage',
  'working hours',
  'layoffs',
  'average wage',
  'deficit'
) 

params$detrend_win = 2

params$smooth_win = 3

params$roll_cor_win = 5

```


```{r load_data}

data_dir_path = paste0(Sys.getenv("USERPROFILE"),
                       "\\OneDrive - Bank Of Israel\\current_work",
                       "\\real estate sentiment\\data\\labor_market")

files_list = list.files(data_dir_path,
                        pattern = "*.rds")

data_df = tibble(name = files_list) %>% 
  mutate(df = map(name, ~import_sentiment_df(paste0(data_dir_path, "\\",.))))

```

```{r summary_stats}

data_df %>% 
  mutate(summary_df = map(df, function(temp_df){
    
    sum_df = tibble(date_min = min(temp_df$date),
                    date_max = max(temp_df$date),
                    unique_articles = length(unique(temp_df$article_id)))
    
    return(sum_df)
    
    
  })) %>% 
  select(name, summary_df) %>% 
  unnest(summary_df) %>% 
  gt()

```

